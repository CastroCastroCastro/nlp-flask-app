{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9938633-83ce-4214-a18a-802ca51d0a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia-api nltk spacy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1383087-680c-4d88-af48-074b0a1bea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0fa4b452-dd99-453f-b9f5-eb133471d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to make requests to wikipedia\n",
    "import wikipediaapi\n",
    "\n",
    "def get_wiki(pageName):\n",
    "    wiki = wikipediaapi.Wikipedia(\n",
    "    language=\"en\",\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "    user_agent=\"'nlpProject(castr385@umn.edu)\"\n",
    "    )\n",
    "    \n",
    "    page = wiki.page(pageName)\n",
    "    if page.exists():\n",
    "        return page.text\n",
    "    else:\n",
    "        print(\"Page Not Found\")\n",
    "        \n",
    "#Test the function / api\n",
    "# title = \"Albert Einstein\"\n",
    "# content = get_wiki(title)\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42c8af-9cc8-4510-b728-5447c0d1acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Albert', 'Einstein', 'physicist'], ['develop', 'theory', 'relativity']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/castro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/castro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/castro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/castro/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "# Main text processing function\n",
    "def process_text(text):\n",
    "    # Step 1: Sentence Segmentation\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Step 2: Tokenization\n",
    "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    # print(\"Tokens:\", tokens)\n",
    "\n",
    "    # Step 3: Stop Word Removal\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [\n",
    "        [word for word in sentence if word.lower() not in stop_words and word.isalpha()]\n",
    "        for sentence in tokens\n",
    "    ]\n",
    "    # print(\"Filtered Tokens:\", filtered_tokens)\n",
    "\n",
    "    # Step 4: POS Tagging and Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for sentence in filtered_tokens:\n",
    "        pos_tagged = nltk.pos_tag(sentence)  # POS tagging\n",
    "        lemmatized_sentence = [\n",
    "            lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tagged\n",
    "        ]\n",
    "        lemmatized_tokens.append(lemmatized_sentence)\n",
    "    \n",
    "    # print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Test the process_text function\n",
    "# sample_text = \"Albert Einstein was a physicist. He developed the theory of relativity.\"\n",
    "# processed = process_text(sample_text)\n",
    "# print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d15f35bd-a0c9-479b-90bc-c7db778ce10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependency Parsing to analyze grammatical relationships between words\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def parse_dependencies(text1):\n",
    "    doc = nlp(text1)\n",
    "    print(\"Dependency Parsing:\")\n",
    "    for token in doc:\n",
    "        print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n",
    "# text = \"Albert Einstein developed the theory of relativity.\"\n",
    "# parse_dependencies(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ebb0b4dc-db33-48ed-b408-bfa18837220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate semantic value\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity_calculator(doc1, doc2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform([doc1, doc2])\n",
    "    similarity_score = cosine_similarity(X[0:1], X[1:2])\n",
    "    return similarity_score[0][0]\n",
    "\n",
    "# doc1 = \"Albert Einstein was a physicist who developed the theory of relativity.\"\n",
    "# doc2 = \"Isaac Newton was a mathematician who developed the laws of motion.\"\n",
    "# similarity = similarity_calculator(doc1, doc2)\n",
    "# print(f\"Similarity Score: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b937d2a-a31e-4e6c-8999-97455936c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Welcome to the Wikipedia Text Similarity Tool!\")\n",
    "    \n",
    "    # Input Wikipedia Titles\n",
    "    title1 = input(\"Enter the title of the first Wikipedia page: \")\n",
    "    title2 = input(\"Enter the title of the second Wikipedia page: \")\n",
    "    \n",
    "    try:\n",
    "        # Fetch Wikipedia Content\n",
    "        text1 = get_wiki(title1)\n",
    "        text2 = get_wiki(title2)\n",
    "\n",
    "        # Print the first 500 characters of each text for sanity check\n",
    "        print(f\"\\nFirst 50 characters of Text 1 ({title1}):\\n{text1[:50]}\")\n",
    "        print(f\"\\nFirst 50 characters of Text 2 ({title2}):\\n{text2[:50]}\")\n",
    "        \n",
    "        # Preprocess the Texts\n",
    "        processed_text1 = process_text(text1)\n",
    "        processed_text2 = process_text(text2)\n",
    "\n",
    "        # Flatten Tokenized Sentences for Similarity Calculation\n",
    "        flat_text1 = \" \".join([\" \".join(sentence) for sentence in processed_text1])\n",
    "        flat_text2 = \" \".join([\" \".join(sentence) for sentence in processed_text2])\n",
    "        \n",
    "        # Dependency Parsing on Raw Text (Optional)\n",
    "        print(\"\\nDependency Parsing for Text 1:\")\n",
    "        parse_dependencies(text1[:50])  # Parse the first 1000 characters of raw text\n",
    "        \n",
    "        print(\"\\nDependency Parsing for Text 2:\")\n",
    "        parse_dependencies(text2[:50])  # Parse the first 1000 characters of raw text\n",
    "        \n",
    "        # Calculate Similarity\n",
    "        similarity_score = similarity_calculator(flat_text1, flat_text2)\n",
    "        print(f\"\\nSimilarity Score between the two pages: {similarity_score:.2f}\")\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
